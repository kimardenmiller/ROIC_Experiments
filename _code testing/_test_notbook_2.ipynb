{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Feature Names: \n ['total yield' 'ms debt' 'roic v2' 'book price' 'sic' 'opInc' 'fcf yield'\n 'comStockEq' 'ms cem' 'total return' 'spitz roic' 'ms ce' 'momentum'] \n [0,6,7,8,9,10,12,13,15,16,17,18,19,20]\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial Feature Names: \\n ['total yield' 'ms debt' 'roic v2' 'book price' 'sic' 'opInc' 'fcf yield'\\n 'comStockEq' 'ms cem' 'total return' 'spitz roic' 'ms ce' 'momentum'] \\n [0,6,7,8,9,10,12,13,15,16,17,18,19,20]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Feature Names: \n ['total yield' 'roic v2' 'book price' 'fcf yield' 'total return'\n 'spitz roic' 'momentum']\nFirst few Stocks with Features, no Lables:  \n [['DD' '0.03' '0.19' '0.16' '0.03' '0.09' '0.18' '0.93']\n ['DOW' '0.04' '0.13' '0.42' '0.03' '0.1' '0.13' '0.94']]  ...\n1436 Stocks by 8 Features\n"
     ]
    }
   ],
   "source": [
    "# Import all tradable stocks of 2015 (features X)\n",
    "import numpy as np\n",
    "import csv\n",
    "data = list(csv.reader(open('data/x2015_noFinance.csv')))\n",
    "feature_data = np.asarray(data)\n",
    "# print('Example Stock with All Features: ', '\\n', feature_data[0:2,:], ' ...')  # First stock in X\n",
    "# First stock in X with selected features\n",
    "selected_features = feature_data[:, [0, 6, 8, 9, 13, 17, 18, 20]]  \n",
    "# print('Example Stock with Selected Features: ', '\\n', selected_features[0:2,:], ' ...')\n",
    "# X with selected features and labels & blanks removed\n",
    "x_data_features = selected_features[1:,:]\n",
    "x_data_features[x_data_features == ''] = 0.0\n",
    "x_data = x_data_features\n",
    "selected_feature_labels = selected_features[0,1:]\n",
    "print('Selected Feature Names: \\n', selected_feature_labels)\n",
    "print('First few Stocks with Features, no Lables: ', '\\n', x_data[0:2,:], ' ...')\n",
    "print(np.size(x_data[:,0]), 'Stocks by', np.size(x_data[0,:]), 'Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV shape:  (1437, 25)\nExample CVS Stock with All Features:  \n [['Ticker' 'Company' 'score' 'Close' 'Market Cap' 'Change 1M' 'total yield'\n  'ms debt' 'roic v2' 'book price' 'sic' 'price251ago' 'opInc' 'fcf yield'\n  'dolVol' 'comStockEq' 'ms cem' 'total return' 'spitz roic' 'ms ce'\n  'momentum' 'Sector' 'Industry' 'Description' 'rebalance']\n ['DD' 'E.I. du Pont de Nemours' '0' '69.26' '58553545945.41' '0.46' '0.03'\n  '11356000156.93' '0.19' '0.16' '16417999988.64' '64.86' '2892999992.35'\n  '0.03' '183223497.72' '9514000033.31' '5531999915.68' '0.09' '0.18'\n  '4452000010.73' '0.93' 'Basic Materials' 'Agriculture'\n  'A science and technology based company' 'False']]  ...\nZero lines: \n ['' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' '' ''\n '' '' '' '' '' '' '' '' '' '' '' '' '' '']\nPanda shape:  (1436, 25)\nPanda shape:  (1437, 25)\nExample Panda Stock with All Features:  \n [['DD' 'E.I. du Pont de Nemours' 0 69.26 58553545945.41 0.46 0.03\n  11356000156.93 0.19 0.16 16417999988.64 64.86 2892999992.35 0.03\n  183223497.72 9514000033.31 5531999915.68 0.09 0.18 4452000010.73 0.93\n  'Basic Materials' 'Agriculture' 'A science and technology based company'\n  False]\n ['DOW' 'Dow Chemical' 0 53.43 58403614653.4 -0.17 0.04 21291999674.27 0.13\n  0.42 39294999803.92 50.1 5032000045.36 0.03 317451662.03 25035000219.44\n  7032000010.63 0.1 0.13 7032000010.63 0.94 'Basic Materials' 'Chemicals'\n  'Manufactures chemical and plastic products' False]]  ...\nSelected Feature Names: \n [0.03 0.19 0.16 0.03 0.09 0.18 0.93]\nFirst few Stocks with Features, no Labels:  \n [['DOW' 0.04 0.13 0.42 0.03 0.1 0.13 0.94]\n ['MON' 0.09 0.2 0.1 0.04 0.1 0.2 0.88]]  ...\n1435 Stocks by 8 Features\nFirst few X Training Examples with 7 Selected Features: \n [[ 0.04  0.13  0.42  0.03  0.1   0.13  0.94]\n [ 0.09  0.2   0.1   0.04  0.1   0.2   0.88]]  ...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-aec6205d7438>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# A)  Standardize the X data (*** But this approach gives negatives, which is not good.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0mstd_rescaledX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# summarize transformed data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kimardenmiller/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kimardenmiller/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    581\u001b[0m         X = check_array(X, accept_sparse=('csr', 'csc'), copy=self.copy,\n\u001b[1;32m    582\u001b[0m                         \u001b[0mensure_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m                         estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kimardenmiller/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    405\u001b[0m                              % (array.ndim, estimator_name))\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mshape_repr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_shape_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kimardenmiller/anaconda/envs/tensorflow/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     56\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     57\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 58\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# Test for Scalar Fail\n",
    "import numpy as np\n",
    "import csv\n",
    "data = list(csv.reader(open('/Users/kimardenmiller/dropbox/tensorflow/data/x2015_noFinance.csv')))\n",
    "csv_data = np.asarray(data)\n",
    "print('CSV shape: ', csv_data.shape)\n",
    "print('Example CVS Stock with All Features: ', '\\n', csv_data[0:2, :], ' ...')  # First stock in X\n",
    "print('Zero lines: \\n', csv_data[csv_data == ''])\n",
    "\n",
    "import pandas\n",
    "x_import = pandas.read_csv('/Users/kimardenmiller/dropbox/tensorflow/data/x2015_noFinance.csv', header=0)\n",
    "print('Panda shape: ', x_import.shape)\n",
    "# print('Example Panda Pre Stock with All Features: ', '\\n', x_import)  # First stock in X\n",
    "x_import.fillna(value=0)\n",
    "# print('Panda Nulls: ', x_import[x_import > 0])\n",
    "panda_data = x_import.values\n",
    "print('Panda shape: ', x_data_values.shape)\n",
    "print('Example Panda Stock with All Features: ', '\\n', panda_data[0:2, :], ' ...')  # First stock in X\n",
    "\n",
    "# First stock in X with selected features\n",
    "selected_features = panda_data[:, [0, 6, 8, 9, 13, 17, 18, 20]]  \n",
    "# print('Example Stock with Selected Features: ', '\\n', selected_features[0:2,:], ' ...')\n",
    "# X with selected features and labels & blanks removed\n",
    "x_data_features = selected_features[1:, :]\n",
    "x_data_features[x_data_features == ''] = 0.0\n",
    "x_data = x_data_features\n",
    "selected_feature_labels = selected_features[0, 1:]\n",
    "print('Selected Feature Names: \\n', selected_feature_labels)\n",
    "print('First few Stocks with Features, no Labels: ', '\\n', x_data[0:2, :], ' ...')\n",
    "print(np.size(x_data[:, 0]), 'Stocks by', np.size(x_data[0,:]), 'Features')\n",
    "x_strings = x_data[:, 1:]  # take off tickers, as they can't be tensor'd\n",
    "raw_X = x_strings.astype(np.float)  # convert strings to float\n",
    "print('First few X Training Examples with', np.size(raw_X[0, :]), 'Selected Features: \\n', raw_X[0:2, :], ' ...')\n",
    "\n",
    "# A)  Standardize the X data (*** But this approach gives negatives, which is not good.)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(raw_X)\n",
    "std_rescaledX = scaler.transform(raw_X)\n",
    "# summarize transformed data\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print('Pre Standardizing: \\n', raw_X[0:5, :])\n",
    "print('After Standardizing: \\n', std_rescaledX[0:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few Positive Examples:  \n [['AAPL' 'Apple, Inc.' 'NASD' 'Technology' 'Computer Hardware']\n ['ABAX' 'ABAXIS, Inc.' 'NASD' 'Health Care' 'Medical Supplies']\n ['ABC' 'AmerisourceBergen Corp.' 'NYSE' 'Consumer Staples'\n  'Drug Retailers']]  ...\nTotal Y Tickers:  237\nTotal Positive Y Ticker Example Count:  119\nTotal Positive Y Ticker Example Count on x_tickers:  119\nFirst few X Training Examples with 7 Selected Features: \n [[ 0.03  0.19  0.16  0.03  0.09  0.18  0.93]\n [ 0.04  0.13  0.42  0.03  0.1   0.13  0.94]]  ...\n"
     ]
    }
   ],
   "source": [
    "# Import best performing stocks of 2015 (y = 1)\n",
    "import csv\n",
    "data = list(csv.reader(open('/Users/kimardenmiller/dropbox/tensorflow/data/y201501_noFinancials.csv')))\n",
    "y_data = np.asarray(data[1:])\n",
    "print('First few Positive Examples: ', '\\n', y_data[0:3,0:5], ' ...')\n",
    "# Find X and Y tickers\n",
    "x_tickers = x_data[:,0]\n",
    "y_tickers = y_data[:,0]\n",
    "print('Total Y Tickers: ', np.size(y_tickers))\n",
    "# Format Y to y = 1 (positive) and y = 0 (negative) examples \n",
    "true_false_mask = np.in1d(x_tickers, y_tickers)\n",
    "y_mask = np.where(true_false_mask,1,0)\n",
    "print('Total Positive Y Ticker Example Count: ', np.size(np.nonzero(y_mask)), )\n",
    "print('Total Positive Y Ticker Example Count on x_tickers: ', np.size(x_tickers[np.nonzero(y_mask)]))\n",
    "# Place dataset into input (X) and output (Y) variables\n",
    "x_strings = x_data[:,1:]  # take off tickers, as they can't be tensor'd\n",
    "raw_X = x_strings.astype(np.float)  # convert strings to float\n",
    "Y = y_mask        # Y uses the 0, 1 to show negative and positive examples\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print('First few X Training Examples with', np.size(raw_X[0,:]) , 'Selected Features: \\n', raw_X[0:2,:], ' ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre Standardizing: \n [[ 0.03  0.19  0.16  0.03  0.09  0.18  0.93]\n [ 0.04  0.13  0.42  0.03  0.1   0.13  0.94]\n [ 0.09  0.2   0.1   0.04  0.1   0.2   0.88]\n [ 0.04  0.13  0.21  0.04 -0.01  0.13  0.92]\n [ 0.03  0.16  0.16  0.04  0.08  0.16  0.96]]\nAfter Standardizing: \n [[-0.424 -0.071 -0.563 -0.003 -0.024 -0.031  0.587]\n [-0.233 -0.122 -0.059 -0.003 -0.024 -0.053  0.662]\n [ 0.72  -0.062 -0.68   0.048 -0.024 -0.022  0.211]\n [-0.233 -0.122 -0.466  0.048 -0.025 -0.053  0.512]\n [-0.424 -0.097 -0.563  0.048 -0.024 -0.04   0.813]]\n"
     ]
    }
   ],
   "source": [
    "# A)  Standardize the X data (*** But this approach gives negatives, which is not good.)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(raw_X)\n",
    "std_rescaledX = scaler.transform(raw_X)\n",
    "# summarize transformed data\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print('Pre Standardizing: \\n', raw_X[0:5, :])\n",
    "print('After Standardizing: \\n', std_rescaledX[0:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre scaling: \n",
      " [[ 0.03  0.19  0.16  0.03  0.09  0.18  0.93]\n",
      " [ 0.04  0.13  0.42  0.03  0.1   0.13  0.94]\n",
      " [ 0.09  0.2   0.1   0.04  0.1   0.2   0.88]\n",
      " [ 0.04  0.13  0.21  0.04 -0.01  0.13  0.92]\n",
      " [ 0.03  0.16  0.16  0.04  0.08  0.16  0.96]]\n",
      "After scaling: \n",
      " [[ 0.049  0.006  0.436  0.484  0.028  0.028  0.922]\n",
      " [ 0.066  0.004  0.457  0.484  0.028  0.028  0.933]\n",
      " [ 0.148  0.006  0.431  0.486  0.028  0.029  0.867]\n",
      " [ 0.066  0.004  0.44   0.486  0.028  0.028  0.911]\n",
      " [ 0.049  0.005  0.436  0.486  0.028  0.028  0.956]]\n"
     ]
    }
   ],
   "source": [
    "# B)  Rescale data (between 0 and 1)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "range_rescaledX = scaler.fit_transform(raw_X)\n",
    "# summarize transformed data\n",
    "np.set_printoptions(precision=3)\n",
    "print('Pre scaling: \\n', raw_X[0:5,:])\n",
    "print('After scaling: \\n', range_rescaledX[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre scaling: \n",
      " [[ 0.03  0.19  0.16  0.03  0.09  0.18  0.93]\n",
      " [ 0.04  0.13  0.42  0.03  0.1   0.13  0.94]\n",
      " [ 0.09  0.2   0.1   0.04  0.1   0.2   0.88]\n",
      " [ 0.04  0.13  0.21  0.04 -0.01  0.13  0.92]\n",
      " [ 0.03  0.16  0.16  0.04  0.08  0.16  0.96]]\n",
      "After scaling: \n",
      " [[ 0.03   0.193  0.163  0.03   0.091  0.183  0.945]\n",
      " [ 0.038  0.124  0.399  0.029  0.095  0.124  0.894]\n",
      " [ 0.096  0.213  0.106  0.043  0.106  0.213  0.936]\n",
      " [ 0.042  0.135  0.218  0.042 -0.01   0.135  0.955]\n",
      " [ 0.03   0.159  0.159  0.04   0.08   0.159  0.957]]\n"
     ]
    }
   ],
   "source": [
    "# C)  Normalize data (length of 1)\n",
    "from sklearn.preprocessing import Normalizer\n",
    "scaler = Normalizer().fit(raw_X)\n",
    "normalizedX = scaler.transform(raw_X)\n",
    "# summarize transformed data\n",
    "np.set_printoptions(precision=3)\n",
    "print('Pre scaling: \\n', raw_X[0:5,:])\n",
    "print('After scaling: \\n', normalizedX[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(7, input_dim=7, init='uniform', activation='relu'))\n",
    "model.add(Dense(7, init='uniform', activation='relu'))\n",
    "model.add(Dense(1, init='uniform', activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre Feature Normalization: \n",
      " [[ 0.03  0.19  0.16  0.03  0.09  0.18  0.93]\n",
      " [ 0.04  0.13  0.42  0.03  0.1   0.13  0.94]\n",
      " [ 0.09  0.2   0.1   0.04  0.1   0.2   0.88]\n",
      " [ 0.04  0.13  0.21  0.04 -0.01  0.13  0.92]\n",
      " [ 0.03  0.16  0.16  0.04  0.08  0.16  0.96]]\n",
      "After Feature Normalization: \n",
      " [[ 0.03  0.19  0.16  0.03  0.09  0.18  0.93]\n",
      " [ 0.04  0.13  0.42  0.03  0.1   0.13  0.94]\n",
      " [ 0.09  0.2   0.1   0.04  0.1   0.2   0.88]\n",
      " [ 0.04  0.13  0.21  0.04 -0.01  0.13  0.92]\n",
      " [ 0.03  0.16  0.16  0.04  0.08  0.16  0.96]]\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', 'fbeta_score'])\n",
    "\n",
    "# Set any Feature Normalization\n",
    "X = raw_X # Choices: std_rescaledX, range_rescaledX, normalizedX or just raw_X\n",
    "\n",
    "print('Pre Feature Normalization: \\n', raw_X[0:5,:])\n",
    "print('After Feature Normalization: \\n', X[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2727 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 2/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2729 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 3/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2723 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 4/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2731 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 5/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2726 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 6/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2732 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 7/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2729 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 8/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2727 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 9/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2732 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 10/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2735 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 11/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2727 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 12/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2730 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 13/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2727 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 14/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2730 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 15/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2724 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 16/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2730 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 17/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2726 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 18/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2729 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 19/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2729 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 20/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2729 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 21/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2735 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 22/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2728 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 23/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2722 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 24/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2731 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 25/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2728 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 26/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2732 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 27/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2727 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 28/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2723 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 29/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2721 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 30/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2729 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 31/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2730 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 32/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2722 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 33/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2729 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 34/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2729 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 35/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2724 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 36/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2732 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 37/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2723 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 38/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2726 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 39/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2727 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 40/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2727 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 41/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2725 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 42/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2722 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 43/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2732 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 44/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2727 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 45/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2732 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 46/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2727 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 47/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2720 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 48/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2729 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 49/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2724 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 50/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2727 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 51/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2719 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 52/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2733 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 53/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2728 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 54/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2721 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 55/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2724 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 56/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2734 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 57/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2721 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 58/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2733 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 59/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2720 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 60/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2729 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 61/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2724 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 62/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2724 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 63/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2724 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 64/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2721 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 65/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2731 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 66/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2722 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 67/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2724 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 68/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2721 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 69/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2726 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 70/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2725 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 71/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2724 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 72/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2726 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 73/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2719 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 74/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2724 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 75/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2716 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 76/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2730 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 77/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2721 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 78/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2724 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 79/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2727 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 80/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2717 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 81/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2727 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 82/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2717 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 83/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2729 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 84/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2719 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 85/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2725 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 86/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2711 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 87/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2732 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 88/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2726 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 89/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2720 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 90/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2723 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 91/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2722 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 92/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2722 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 93/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2720 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 94/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2723 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 95/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2723 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 96/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2716 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 97/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2724 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 98/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2721 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 99/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2724 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 100/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2717 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 101/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2723 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 102/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2722 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 103/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2723 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 104/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2724 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 105/600\n",
      "1436/1436 [==============================] - 1s - loss: 0.2717 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 106/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2720 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 107/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2716 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 108/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2726 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 109/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2722 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 110/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2721 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 111/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2720 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 112/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2720 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 113/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2722 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 114/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2721 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 115/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2721 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 116/600\n",
      "1436/1436 [==============================] - 1s - loss: 0.2718 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 117/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2724 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 118/600\n",
      "1436/1436 [==============================] - 1s - loss: 0.2722 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 119/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2720 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 120/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2718 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 121/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2726 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 122/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2719 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 123/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2717 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 124/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2726 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 125/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2712 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 126/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2730 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 127/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2714 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 128/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2723 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 129/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2716 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 130/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2718 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 131/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2729 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 132/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2718 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 133/600\n",
      "1436/1436 [==============================] - 1s - loss: 0.2724 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 134/600\n",
      "1436/1436 [==============================] - 1s - loss: 0.2723 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 135/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2714 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 136/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2715 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 137/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2719 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 138/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2722 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 139/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2715 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 140/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2721 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 141/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2708 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 142/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2721 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 143/600\n",
      "1436/1436 [==============================] - 1s - loss: 0.2717 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 144/600\n",
      "1436/1436 [==============================] - 1s - loss: 0.2728 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 145/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2716 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 146/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2722 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 147/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2717 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 148/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2719 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 149/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2715 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 150/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2721 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 151/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2722 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 152/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2707 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 153/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2729 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 154/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2718 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 155/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2719 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 156/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2715 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 157/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2719 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 158/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2721 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 159/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2720 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 160/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2716 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 161/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2719 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 162/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2719 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 163/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2721 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 164/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2715 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 165/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2718 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 166/600\n",
      "1436/1436 [==============================] - 1s - loss: 0.2713 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 167/600\n",
      "1436/1436 [==============================] - 1s - loss: 0.2713 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 168/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2727 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 169/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2715 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 170/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2721 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 171/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2712 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 172/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2717 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 173/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2716 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 174/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2717 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 175/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2718 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 176/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2712 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 177/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2693 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 452/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2685 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 453/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2691 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 454/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2693 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 455/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2687 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 456/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2689 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 457/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2693 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 458/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2690 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 459/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2697 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 460/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2684 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 461/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2690 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 462/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2691 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 463/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2683 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 464/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2695 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 465/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2675 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 466/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2695 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 467/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2689 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 468/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2687 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 469/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2691 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 470/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2683 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 471/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2692 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 472/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2693 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 473/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2686 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 474/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2690 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 475/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2691 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 476/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2686 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 477/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2689 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 478/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2688 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 479/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2686 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 480/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2680 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 481/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2686 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 482/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2688 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 483/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2688 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 484/600\n",
      "1436/1436 [==============================] - 0s - loss: 0.2688 - acc: 0.9171 - fbeta_score: nan     \n",
      "Epoch 485/600\n",
      "1420/1436 [============================>.] - ETA: 0s - loss: 0.2684 - acc: 0.9176 - fbeta_score: nan"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X, Y, nb_epoch=600, batch_size=10)\n",
    "# model.fit(normalizedX, Y, nb_epoch=300, batch_size=10) # Change back to 150 after testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1184/1436 [=======================>......] - ETA: 0s\n",
      " loss: 26.56%\n",
      "acc: 91.71%\n",
      "fbeta_score: nan%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X, Y)\n",
    "# scores = model.evaluate(normalizedX, Y)\n",
    "print('\\n', \"%s: %.2f%%\" % (model.metrics_names[0], scores[0]*100))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[2], scores[2]*100))\n",
    "# print(model.metrics_names)\n",
    "# print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fist 100 Predictions:  \n",
      " [ 0.086  0.075  0.13   0.075  0.074  0.079  0.115  0.117  0.087  0.046\n",
      "  0.097  0.056  0.057  0.072  0.013  0.099  0.073  0.12   0.055  0.055\n",
      "  0.041  0.093  0.032  0.079  0.074  0.087  0.055  0.09   0.06   0.053\n",
      "  0.027  0.044  0.042  0.09   0.128  0.044  0.122  0.056  0.036  0.045\n",
      "  0.077  0.039  0.067  0.055  0.047  0.075  0.036  0.085  0.111  0.045\n",
      "  0.068  0.064  0.022  0.042  0.22   0.063  0.067  0.048  0.049  0.096\n",
      "  0.028  0.075  0.14   0.043  0.081  0.173  0.062  0.072  0.08   0.103\n",
      "  0.097  0.057  0.032  0.087  0.057  0.109  0.013  0.08   0.06   0.072\n",
      "  0.086  0.079  0.078  0.047  0.096  0.     0.026  0.107  0.047  0.044\n",
      "  0.011  0.026  0.039  0.071  0.016  0.038  0.062  0.     0.053  0.045]\n",
      "Rows in Prediction:  1436\n",
      "Positive Predictions:  0\n",
      "Positive Prediction Pointers:  \n",
      " (array([], dtype=int64),)\n",
      "Positive Prediction Tickers:  \n",
      " []\n",
      "Positive Prediction Tickers:  \n",
      " []\n",
      "Positive Prediction Ground Truth:  \n",
      " []\n",
      "Accuracy of Positive Predictions:  \n",
      " 0.0%\n"
     ]
    }
   ],
   "source": [
    "# calculate predictions\n",
    "predictions = model.predict(X)\n",
    "# predictions = model.predict(normalizedX)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "print('Fist 100 Predictions: ', '\\n', predictions[0:100,0])\n",
    "print('Rows in Prediction: ', np.size(predictions[:,0]))\n",
    "positive_predictions = np.sum(predictions[:,0] > .5)\n",
    "print('Positive Predictions: ', positive_predictions)\n",
    "print('Positive Prediction Pointers: ', '\\n', np.where(predictions[:,0] > .5))\n",
    "print('Positive Prediction Tickers: ', '\\n', x_tickers[np.where(predictions[:,0] > .5)])\n",
    "\n",
    "picks_data = feature_data[1:,:]\n",
    "print('Positive Prediction Tickers: ', '\\n', picks_data[np.where(predictions[:,0] > .5),0:2][0] )\n",
    "print('Positive Prediction Ground Truth: ', '\\n', Y[np.where(predictions[:,0] > .5)] )\n",
    "accurate_predictions = np.size(np.nonzero(Y[np.where(predictions[:,0] > .5)]))\n",
    "print('Accuracy of Positive Predictions: ', '\\n', \"%.1f%%\" % ((accurate_predictions / positive_predictions) * 100 if positive_predictions > 0 else 0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {
   "attach-environment": true,
   "summary": "Keras Sequential"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}